# 사용자 가이드


참고)

>google chrome
: https://velog.io/@choi-yh/Ubuntu-20.04-selinium-%EC%84%B8%ED%8C%85
 사용한 버전: Google Chrome 106.0.5249.103 <br>
gnu parallel
: https://installati.one/ubuntu/20.04/parallel/   <br>
tor proxy
: https://wkdtjsgur100.github.io/selenium-change-ip/
<!-- * 파일 실행 테스트 과정: tor_test.py > split_urls.py > yes24_newBook_list.py > url_crawler.py > : -->

<br><br><br>


<h3>패키지 설치</h3>
*버전확인: $ pip3 show [패키지명] 


```	
$ pip3 install bs4  //beautifulsoup4-4.11.1 ,  bs4-0.0.1
$ pip3 install selenium  //selenium-4.6.0 
$ pip3 install pymysql  //pymysql-1.0.2
$ pip3 install logger  // logger-1.4
$ pip3 install pandas  // pandas-1.5.1
$ pip3 install tqdm  // tqdm-4.64.1
$ pip3 install webdriver-manager  // webdriver-manager-3.8.5
$ sudo apt-get install parallel  //gnu parallel
```	
<br><br>

  <h3>IP 우회 프로그램 설치 (Tor)</h3>

```	$ sudo apt-get install tor
$ pip3 install requests[socks]
$ sudo /etc/init.d/tor start
```	

tor ip 변경 주기 더 빠르게 설정(default: 10분) :<br>
    tor config file(torrc) 수정<br>
    참고> https://www.reddit.com/r/TOR/comments/81xjk7/change_ipaddress_automatically_in_your_interval/

```	
$ cd /etc/tor
$ sudo vi torrc
$ sudo pkill -sighup tor  //reload tor after editing torrc
```	

<br><br><br>



<h3> 웹 접근을 위한 web driver 설치</h3>

```	
$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
$ sudo apt install ./google-chrome-stable_current_amd64.deb
$ google-chrome --version   // 107.0.5304.110
크롬 버전과 맞는 버전의 크롬드라이버 설치 (참고: https://chromedriver.chromium.org/downloads/version-selection)
$ sudo wget https://chromedriver.storage.googleapis.com/107.0.5304.62/chromedriver_linux64.zip (=zip파일 링크)
$ unzip chromedriver_linux64.zip 
```	

*필요시 소스의 driver 경로인 executablepath 수정




<br><br><br><br>

  <h3>쉘스크립트 설명 및 실행</h3>


1) init.sh <br>
  - 전체 ebook url과 도서 상세정보 수집 (최초 1회).<br>
  - url_crawler.py(전체 ebook url 수집 후 yes24_url db에 입력) > split_urls.py(yes24_url db에서 도서 url 추출하여 csv로 저장) > crawl_parallel.py(도서 상세정보 수집 후 yes24 db 입력)

  - crawl_parallel.py실행은 gnu parallel로 병렬처리
  
      ```
      parallel -a split_urls.csv -j 5 python3 crawl_parallel.py 1>> ${log_path}init.log 2>> ${log_path}init.err.log

      ```
    &rarr;  split_urls.csv를 crawl_parallel.py의 인자로, job 5개 실행한다. 각각의 job은 csv의 한 행씩을 인자로 받아들인다. crawl_parallel의 출력은 init.log 파일에 append하고, 에러메세지는 init.err.log 파일에 append 한다.  
    
  - 스크립트 nohup으로 실행:
    ```
    $ nohup python3 init.sh 
    ```
    <br>  <br> 

  2) new.sh 
  - 신규도서 수집 스케쥴에 맞춰 신규도서 url 수집, 도서 상세정보 수집.
  - yes24_newBook_list.py(신규도서 url 수집 후 yes24_url db에 입력) > split_urls.py(yes24_url db에서 도서 url 추출하여 csv로 저장) > crawl_parallel.py(도서 상세정보 수집 후 yes24 db 입력)
   - 스크립트 crontab에 등록하여 실행 (아래 참고)
  <br><br><br>


3) update.sh 
  -  도서정보 업데이트 일정에 맞춰, 차후 요구사항에 따라 업데이트 대상 도서만 도서 상세정보 업데이트 진행.
  -  split_update_urls.py(yes24 db에서 도서 url 추출하여 csv로 저장) > crawl_update.py(도서 상세정보 수집 후 yes24 db 업데이트)







<br><br><br>

  <h3>크롤링을 위한 스케쥴러 설정</h3>
  cron 활성화 

```	
$ sudo service cron start
```	

crontab 사용방법

```	
$ crontab -l  // 크론탭에 등록된 내용 확인
$ crontab -e // 크론탭에 실행명령어 등록 및 수정


    - 주기설정:  *　　　　　　*　　　　　　*　　　　　　*　　　　　　*
              분(0-59)　　시간(0-23)　　일(1-31)　　월(1-12)　　　요일(0-7) (1:월요일, 0,7:일요일)


    - 실행명령어 등록: * * * * * [명령어]

    ex) 매월 1일 오전 2에 new.sh 실행 :
         0 2 1 * * /new.sh 
```	
    
참고) https://man7.org/linux/man-pages/man5/crontab.5.html#DESCRIPTION






